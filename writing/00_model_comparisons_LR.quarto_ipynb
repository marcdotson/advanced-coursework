{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Aggregate Logistic Regression in PyMC vs Sklearn: A Step-by-Step Comparison\"\n",
        "format: html\n",
        "---\n",
        "\n",
        "\n",
        "In this tutorial, we will walk through the process of performing logistic regression using two different approaches: Scikit-learn (sklearn) and PyMC. The goal is to demonstrate how both methods work, compare their results, and show how you can present visualizations using Arviz for Bayesian modeling in PyMC.\n",
        "\n",
        "We'll cover:\n",
        "\n",
        "- **Data generation**: Creating a synthetic dataset for testing.\n",
        "- **Modeling**: Running Logistic Regression with both Scikit-learn and PyMC.\n",
        "- **Evaluation**: Evaluating and comparing model performance.\n",
        "- **Visualizations**: Presenting results with useful plots using Arviz for PyMC.\n",
        "\n",
        "# Core libraries\n",
        "\n",
        "These core libraries are crucial for data analysis and modeling. Numpy handles numerical data, Pandas is for data manipulation, and Pymc supports probabilistic modeling. Arviz helps visualize Bayesian model results, while Matplotlib.pyplot is used for plotting. Sklearn offers tools for splitting data, applying machine learning models like logistic regression, and evaluating performance with metrics like accuracy, confusion matrix, and ROC curves.\n"
      ],
      "id": "bcce1359"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc"
      ],
      "id": "05e56546",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Data Generation\n",
        "\n",
        "We'll start by generating synthetic data to test both models. The data consists of two features (X1, X2) and a target variable (Y), where Y is based on a logistic function of the features.\n"
      ],
      "id": "42633cf2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate random data\n",
        "n_samples = 200\n",
        "X1 = np.random.normal(loc=0, scale=1, size=n_samples)  # Feature 1\n",
        "X2 = np.random.normal(loc=2, scale=1.5, size=n_samples)  # Feature 2\n",
        "\n",
        "# Create a simple rule for the target variable\n",
        "Y = (X1 + X2 + np.random.normal(scale=0.5, size=n_samples) > 2).astype(int)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'X1': X1, 'X2': X2, 'Y': Y})\n",
        "\n",
        "print(df.head())"
      ],
      "id": "1a585df2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, we are creating two random features (X1, X2) and using a simple rule with noise to generate the binary target variable (Y). This is a typical setup for logistic regression problems.\n",
        "\n",
        "# Step 2: Logistic Regression using Scikit-learn\n",
        "\n",
        "Scikit-learn provides an easy way to perform logistic regression using the LogisticRegression class. First, we’ll split the data into training and testing sets, train the model, and evaluate its performance.\n"
      ],
      "id": "fb3e7d72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = df[['X1', 'X2']]\n",
        "y = df['Y']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression Model with Scikit-learn\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "id": "2657e984",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We evaluate the model using accuracy and a classification report that includes precision, recall, and F1-score.\n",
        "\n",
        "## Visualize Scikit-learn Results\n"
      ],
      "id": "58fadf84"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, log_reg.predict_proba(X_test)[:, 1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "id": "e131b6e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This visual will allow us to assess the model’s performance in terms of its ability to distinguish between the two classes.\n",
        "\n",
        "\n",
        "# Step 3: Logistic Regression using PyMC\n",
        "\n",
        "Now, let's implement logistic regression using PyMC, a Bayesian modeling framework. We will build a probabilistic model that fits the data and then sample from the posterior distribution of the parameters.\n",
        "\n",
        "# Your data generation code here (or loading from a file)"
      ],
      "id": "1d76ceb0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(42)\n",
        "X1 = np.random.normal(0, 1, 100)\n",
        "X2 = np.random.normal(0, 1, 100)\n",
        "y = (1 / (1 + np.exp(-(1 + 2 * X1 - 3 * X2)))) > 0.5\n",
        "y = y.astype(int)\n",
        "\n",
        "data = pd.DataFrame({'X1': X1, 'X2': X2, 'y': y})\n",
        "\n",
        "def run_logistic_regression():\n",
        "    # Define the model\n",
        "    with pm.Model() as logistic_model:\n",
        "        intercept = pm.Normal('intercept', mu=0, sigma=10)\n",
        "        coef_X1 = pm.Normal('coef_X1', mu=0, sigma=10)\n",
        "        coef_X2 = pm.Normal('coef_X2', mu=0, sigma=10)\n",
        "\n",
        "        linear_model = intercept + coef_X1 * data['X1'] + coef_X2 * data['X2']\n",
        "\n",
        "        p = pm.math.sigmoid(linear_model)\n",
        "        likelihood = pm.Bernoulli('likelihood', p=p, observed=data['y'])\n",
        "\n",
        "        trace = pm.sample(2000, return_inferencedata=True)\n",
        "\n",
        "    # Traceplot and summary\n",
        "    az.plot_trace(trace)\n",
        "    print(az.summary(trace))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_logistic_regression()\n"
      ],
      "id": "57596612",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Results in PyMC\n",
        "\n",
        "Arviz provides a suite of functions to visualize the results from Bayesian models. After sampling, we can use az.plot_trace() to view the posterior distributions of the model parameters.\n"
      ],
      "id": "9d5f14db"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.plot_trace(trace)"
      ],
      "id": "4f865a7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This plot shows the posterior distributions for each parameter in the model, giving insights into their uncertainty and estimated values.\n",
        "\n",
        "We can also use Arviz to visualize the posterior predictive checks or the model fit.\n",
        "\n",
        "# Step 4: Comparing the Models\n",
        "\n",
        "Now that we’ve trained and evaluated both models, let’s compare them. Here are some key differences:\n",
        "\n",
        "- **Sklearn**: A frequentist approach, estimates fixed coefficients without incorporating uncertainty.\n",
        "- **PyMC**: A Bayesian approach, where we model the uncertainty of the parameters and generate a posterior distribution for each.\n",
        "\n",
        "## Model Comparison - Key Points:\n",
        "\n",
        "- **Interpretation**: In PyMC, we interpret the posterior distributions of the parameters, while in Scikit-learn, we work directly with point estimates.\n",
        "- **Uncertainty**: PyMC allows us to capture the uncertainty in model parameters, which is helpful in complex problems.\n",
        "- **Evaluation**: Both models can be evaluated using metrics like accuracy, but PyMC also offers the advantage of uncertainty quantification through the posterior distributions.\n",
        "\n",
        "## Final Visual Comparison (ROC)\n",
        "\n",
        "Here, we can compare the ROC curves of both models:\n",
        "\n",
        "- **Sklearn**: Generates a traditional ROC curve.\n",
        "- **PyMC**: Since we have uncertainty in the model parameters, we can sample multiple predictions and visualize the spread.\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "This walkthrough demonstrated how to implement logistic regression using Scikit-learn and PyMC. We compared both approaches, highlighting their key differences, and visualized results using standard metrics as well as Arviz for Bayesian modeling.\n",
        "\n",
        "In Scikit-learn, we focused on efficiency and simplicity, while in PyMC, we explored the flexibility of Bayesian models and how they allow us to quantify uncertainty in the predictions.\n"
      ],
      "id": "66e5d648"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\rhigb\\Documents\\GitHub\\advanced-coursework\\.venv\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}